{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week1 cifar10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chupibk/INT3121-20/blob/master/week1-colab-cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIrB3D0MOXJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f5601410-73f8-4454-819c-c0fb826da179"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "#just to be sure, reload the dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "print(\"x_train shape: {}, y_train shape: {}\".format(x_train.shape, y_train.shape))\n",
        "print(\"{} train samples, {} test samples\".format(x_train.shape[0], x_test.shape[0]))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3), y_train shape: (50000, 1)\n",
            "50000 train samples, 10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgFZPy0vOet_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert class vectors to binary class matrices\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdRvoNnMOuc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "e0096b45-d8a4-46b5-fed5-091231c0187f"
      },
      "source": [
        "#build a model\n",
        "# reference: https://keras.io/examples/cifar10_cnn/\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0827 08:44:55.840609 140471924877184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0827 08:44:56.135732 140471924877184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0827 08:44:56.143472 140471924877184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0827 08:44:56.184844 140471924877184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0827 08:44:56.189005 140471924877184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0827 08:44:56.198682 140471924877184 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0827 08:44:56.330452 140471924877184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0827 08:44:56.340492 140471924877184 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA1WDvoYOx72",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "1eaa3d0b-efbc-4328-9811-e1ad0a41778b"
      },
      "source": [
        "# train the model\n",
        "import time\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "start = time.time()\n",
        "# train the model\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "start = time.time()\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=20,\n",
        "                    validation_split=0.2,\n",
        "                    shuffle=True,\n",
        "                    verbose=1\n",
        ")\n",
        "\n",
        "print(\"training time: \", time.time() - start)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0827 08:45:24.796638 140471924877184 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "40000/40000 [==============================] - 22s 541us/step - loss: 2.1633 - acc: 0.2020 - val_loss: 2.1406 - val_acc: 0.2197\n",
            "Epoch 2/20\n",
            "40000/40000 [==============================] - 15s 381us/step - loss: 2.0225 - acc: 0.2646 - val_loss: 1.9573 - val_acc: 0.2984\n",
            "Epoch 3/20\n",
            "40000/40000 [==============================] - 15s 379us/step - loss: 1.9622 - acc: 0.2898 - val_loss: 1.9531 - val_acc: 0.2934\n",
            "Epoch 4/20\n",
            "40000/40000 [==============================] - 15s 380us/step - loss: 1.9129 - acc: 0.3133 - val_loss: 1.8535 - val_acc: 0.3350\n",
            "Epoch 5/20\n",
            "40000/40000 [==============================] - 15s 383us/step - loss: 1.8557 - acc: 0.3329 - val_loss: 1.7833 - val_acc: 0.3615\n",
            "Epoch 6/20\n",
            "40000/40000 [==============================] - 15s 382us/step - loss: 1.7938 - acc: 0.3579 - val_loss: 1.7298 - val_acc: 0.3806\n",
            "Epoch 7/20\n",
            "40000/40000 [==============================] - 15s 380us/step - loss: 1.7393 - acc: 0.3765 - val_loss: 1.6780 - val_acc: 0.4014\n",
            "Epoch 8/20\n",
            "40000/40000 [==============================] - 15s 385us/step - loss: 1.6908 - acc: 0.3942 - val_loss: 1.6357 - val_acc: 0.4188\n",
            "Epoch 9/20\n",
            "40000/40000 [==============================] - 15s 382us/step - loss: 1.6488 - acc: 0.4082 - val_loss: 1.6084 - val_acc: 0.4246\n",
            "Epoch 10/20\n",
            "40000/40000 [==============================] - 15s 381us/step - loss: 1.6109 - acc: 0.4222 - val_loss: 1.5536 - val_acc: 0.4434\n",
            "Epoch 11/20\n",
            "40000/40000 [==============================] - 15s 382us/step - loss: 1.5804 - acc: 0.4358 - val_loss: 1.5380 - val_acc: 0.4489\n",
            "Epoch 12/20\n",
            "40000/40000 [==============================] - 15s 381us/step - loss: 1.5475 - acc: 0.4489 - val_loss: 1.4862 - val_acc: 0.4738\n",
            "Epoch 13/20\n",
            "40000/40000 [==============================] - 15s 382us/step - loss: 1.5203 - acc: 0.4592 - val_loss: 1.4670 - val_acc: 0.4785\n",
            "Epoch 14/20\n",
            "40000/40000 [==============================] - 15s 380us/step - loss: 1.4973 - acc: 0.4634 - val_loss: 1.4312 - val_acc: 0.4915\n",
            "Epoch 15/20\n",
            "40000/40000 [==============================] - 15s 381us/step - loss: 1.4744 - acc: 0.4733 - val_loss: 1.4290 - val_acc: 0.4878\n",
            "Epoch 16/20\n",
            "40000/40000 [==============================] - 15s 385us/step - loss: 1.4527 - acc: 0.4844 - val_loss: 1.3876 - val_acc: 0.5063\n",
            "Epoch 17/20\n",
            "40000/40000 [==============================] - 15s 381us/step - loss: 1.4334 - acc: 0.4858 - val_loss: 1.3685 - val_acc: 0.5170\n",
            "Epoch 18/20\n",
            "40000/40000 [==============================] - 15s 386us/step - loss: 1.4107 - acc: 0.4967 - val_loss: 1.3829 - val_acc: 0.5142\n",
            "Epoch 19/20\n",
            "40000/40000 [==============================] - 15s 376us/step - loss: 1.3983 - acc: 0.5003 - val_loss: 1.3439 - val_acc: 0.5214\n",
            "Epoch 20/20\n",
            "40000/40000 [==============================] - 15s 377us/step - loss: 1.3807 - acc: 0.5077 - val_loss: 1.3535 - val_acc: 0.5204\n",
            "training time:  312.0184247493744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBJTeQuyQB2j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6b520af9-c8f7-4ff5-c823-e3bb4980a9c6"
      },
      "source": [
        "start = time.time()\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=32,\n",
        "                    epochs=100,\n",
        "                    validation_split=0.2,\n",
        "                    shuffle=True,\n",
        "                    verbose=1\n",
        ")\n",
        "\n",
        "print(\"training time: \", time.time() - start)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "40000/40000 [==============================] - 16s 395us/step - loss: 1.3676 - acc: 0.5139 - val_loss: 1.3141 - val_acc: 0.5362\n",
            "Epoch 2/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 1.3526 - acc: 0.5196 - val_loss: 1.3156 - val_acc: 0.5344\n",
            "Epoch 3/100\n",
            "40000/40000 [==============================] - 16s 392us/step - loss: 1.3433 - acc: 0.5220 - val_loss: 1.2843 - val_acc: 0.5468\n",
            "Epoch 4/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 1.3275 - acc: 0.5300 - val_loss: 1.3047 - val_acc: 0.5424\n",
            "Epoch 5/100\n",
            "40000/40000 [==============================] - 16s 392us/step - loss: 1.3154 - acc: 0.5328 - val_loss: 1.2587 - val_acc: 0.5582\n",
            "Epoch 6/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 1.3077 - acc: 0.5342 - val_loss: 1.2818 - val_acc: 0.5462\n",
            "Epoch 7/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 1.2940 - acc: 0.5425 - val_loss: 1.2925 - val_acc: 0.5500\n",
            "Epoch 8/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 1.2798 - acc: 0.5457 - val_loss: 1.2521 - val_acc: 0.5548\n",
            "Epoch 9/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 1.2728 - acc: 0.5497 - val_loss: 1.2354 - val_acc: 0.5627\n",
            "Epoch 10/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 1.2591 - acc: 0.5570 - val_loss: 1.2284 - val_acc: 0.5666\n",
            "Epoch 11/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 1.2504 - acc: 0.5612 - val_loss: 1.2022 - val_acc: 0.5804\n",
            "Epoch 12/100\n",
            "40000/40000 [==============================] - 16s 388us/step - loss: 1.2435 - acc: 0.5616 - val_loss: 1.2008 - val_acc: 0.5792\n",
            "Epoch 13/100\n",
            "40000/40000 [==============================] - 15s 387us/step - loss: 1.2317 - acc: 0.5639 - val_loss: 1.1958 - val_acc: 0.5856\n",
            "Epoch 14/100\n",
            "40000/40000 [==============================] - 15s 386us/step - loss: 1.2250 - acc: 0.5661 - val_loss: 1.2078 - val_acc: 0.5870\n",
            "Epoch 15/100\n",
            "40000/40000 [==============================] - 15s 387us/step - loss: 1.2149 - acc: 0.5729 - val_loss: 1.1854 - val_acc: 0.5875\n",
            "Epoch 16/100\n",
            "40000/40000 [==============================] - 15s 387us/step - loss: 1.2060 - acc: 0.5781 - val_loss: 1.1794 - val_acc: 0.5916\n",
            "Epoch 17/100\n",
            "40000/40000 [==============================] - 15s 386us/step - loss: 1.1978 - acc: 0.5767 - val_loss: 1.2024 - val_acc: 0.5897\n",
            "Epoch 18/100\n",
            "40000/40000 [==============================] - 15s 385us/step - loss: 1.1876 - acc: 0.5826 - val_loss: 1.1494 - val_acc: 0.6033\n",
            "Epoch 19/100\n",
            "40000/40000 [==============================] - 15s 386us/step - loss: 1.1835 - acc: 0.5847 - val_loss: 1.1560 - val_acc: 0.6001\n",
            "Epoch 20/100\n",
            "40000/40000 [==============================] - 15s 384us/step - loss: 1.1774 - acc: 0.5857 - val_loss: 1.1468 - val_acc: 0.6041\n",
            "Epoch 21/100\n",
            "40000/40000 [==============================] - 15s 385us/step - loss: 1.1693 - acc: 0.5921 - val_loss: 1.1254 - val_acc: 0.6063\n",
            "Epoch 22/100\n",
            "40000/40000 [==============================] - 15s 384us/step - loss: 1.1603 - acc: 0.5942 - val_loss: 1.1873 - val_acc: 0.5916\n",
            "Epoch 23/100\n",
            "40000/40000 [==============================] - 15s 386us/step - loss: 1.1544 - acc: 0.5950 - val_loss: 1.1060 - val_acc: 0.6189\n",
            "Epoch 24/100\n",
            "40000/40000 [==============================] - 15s 384us/step - loss: 1.1466 - acc: 0.5970 - val_loss: 1.1048 - val_acc: 0.6190\n",
            "Epoch 25/100\n",
            "40000/40000 [==============================] - 15s 386us/step - loss: 1.1406 - acc: 0.6013 - val_loss: 1.1062 - val_acc: 0.6164\n",
            "Epoch 26/100\n",
            "40000/40000 [==============================] - 15s 386us/step - loss: 1.1293 - acc: 0.6033 - val_loss: 1.1537 - val_acc: 0.6091\n",
            "Epoch 27/100\n",
            "40000/40000 [==============================] - 15s 382us/step - loss: 1.1250 - acc: 0.6078 - val_loss: 1.0663 - val_acc: 0.6253\n",
            "Epoch 28/100\n",
            "40000/40000 [==============================] - 15s 387us/step - loss: 1.1220 - acc: 0.6093 - val_loss: 1.0950 - val_acc: 0.6241\n",
            "Epoch 29/100\n",
            "40000/40000 [==============================] - 16s 388us/step - loss: 1.1149 - acc: 0.6085 - val_loss: 1.0743 - val_acc: 0.6287\n",
            "Epoch 30/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 1.1054 - acc: 0.6167 - val_loss: 1.0486 - val_acc: 0.6418\n",
            "Epoch 31/100\n",
            "40000/40000 [==============================] - 16s 396us/step - loss: 1.1019 - acc: 0.6176 - val_loss: 1.0807 - val_acc: 0.6257\n",
            "Epoch 32/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 1.0987 - acc: 0.6187 - val_loss: 1.0667 - val_acc: 0.6359\n",
            "Epoch 33/100\n",
            "40000/40000 [==============================] - 16s 388us/step - loss: 1.0954 - acc: 0.6199 - val_loss: 1.0716 - val_acc: 0.6298\n",
            "Epoch 34/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 1.0835 - acc: 0.6241 - val_loss: 1.0662 - val_acc: 0.6388\n",
            "Epoch 35/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 1.0819 - acc: 0.6244 - val_loss: 1.0513 - val_acc: 0.6425\n",
            "Epoch 36/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 1.0794 - acc: 0.6245 - val_loss: 1.0257 - val_acc: 0.6495\n",
            "Epoch 37/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 1.0693 - acc: 0.6287 - val_loss: 1.0691 - val_acc: 0.6468\n",
            "Epoch 38/100\n",
            "40000/40000 [==============================] - 16s 388us/step - loss: 1.0671 - acc: 0.6298 - val_loss: 1.0184 - val_acc: 0.6497\n",
            "Epoch 39/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 1.0628 - acc: 0.6333 - val_loss: 1.0119 - val_acc: 0.6532\n",
            "Epoch 40/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 1.0562 - acc: 0.6346 - val_loss: 1.0223 - val_acc: 0.6494\n",
            "Epoch 41/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 1.0573 - acc: 0.6351 - val_loss: 1.0277 - val_acc: 0.6519\n",
            "Epoch 42/100\n",
            "40000/40000 [==============================] - 16s 388us/step - loss: 1.0518 - acc: 0.6361 - val_loss: 1.0034 - val_acc: 0.6610\n",
            "Epoch 43/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 1.0460 - acc: 0.6388 - val_loss: 1.0240 - val_acc: 0.6529\n",
            "Epoch 44/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 1.0469 - acc: 0.6378 - val_loss: 1.0248 - val_acc: 0.6646\n",
            "Epoch 45/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 1.0369 - acc: 0.6437 - val_loss: 0.9876 - val_acc: 0.6601\n",
            "Epoch 46/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 1.0386 - acc: 0.6407 - val_loss: 0.9834 - val_acc: 0.6621\n",
            "Epoch 47/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 1.0338 - acc: 0.6432 - val_loss: 0.9754 - val_acc: 0.6629\n",
            "Epoch 48/100\n",
            "40000/40000 [==============================] - 16s 394us/step - loss: 1.0312 - acc: 0.6456 - val_loss: 1.0601 - val_acc: 0.6413\n",
            "Epoch 49/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 1.0286 - acc: 0.6473 - val_loss: 0.9838 - val_acc: 0.6626\n",
            "Epoch 50/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 1.0270 - acc: 0.6479 - val_loss: 0.9936 - val_acc: 0.6581\n",
            "Epoch 51/100\n",
            "40000/40000 [==============================] - 16s 397us/step - loss: 1.0201 - acc: 0.6484 - val_loss: 1.0043 - val_acc: 0.6561\n",
            "Epoch 52/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 1.0187 - acc: 0.6496 - val_loss: 0.9589 - val_acc: 0.6752\n",
            "Epoch 53/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 1.0109 - acc: 0.6516 - val_loss: 0.9724 - val_acc: 0.6721\n",
            "Epoch 54/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 1.0088 - acc: 0.6552 - val_loss: 0.9871 - val_acc: 0.6646\n",
            "Epoch 55/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 1.0092 - acc: 0.6541 - val_loss: 0.9626 - val_acc: 0.6783\n",
            "Epoch 56/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 1.0028 - acc: 0.6558 - val_loss: 0.9530 - val_acc: 0.6748\n",
            "Epoch 57/100\n",
            "40000/40000 [==============================] - 16s 392us/step - loss: 0.9989 - acc: 0.6569 - val_loss: 0.9762 - val_acc: 0.6746\n",
            "Epoch 58/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 1.0033 - acc: 0.6552 - val_loss: 1.0192 - val_acc: 0.6520\n",
            "Epoch 59/100\n",
            "40000/40000 [==============================] - 16s 393us/step - loss: 1.0001 - acc: 0.6611 - val_loss: 1.0270 - val_acc: 0.6474\n",
            "Epoch 60/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 0.9896 - acc: 0.6595 - val_loss: 0.9605 - val_acc: 0.6699\n",
            "Epoch 61/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9960 - acc: 0.6589 - val_loss: 0.9643 - val_acc: 0.6697\n",
            "Epoch 62/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9895 - acc: 0.6627 - val_loss: 0.9577 - val_acc: 0.6699\n",
            "Epoch 63/100\n",
            "40000/40000 [==============================] - 16s 388us/step - loss: 0.9900 - acc: 0.6604 - val_loss: 0.9793 - val_acc: 0.6664\n",
            "Epoch 64/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9910 - acc: 0.6613 - val_loss: 1.0106 - val_acc: 0.6558\n",
            "Epoch 65/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9857 - acc: 0.6613 - val_loss: 0.9480 - val_acc: 0.6759\n",
            "Epoch 66/100\n",
            "40000/40000 [==============================] - 16s 392us/step - loss: 0.9803 - acc: 0.6638 - val_loss: 0.9390 - val_acc: 0.6788\n",
            "Epoch 67/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 0.9847 - acc: 0.6631 - val_loss: 0.9702 - val_acc: 0.6702\n",
            "Epoch 68/100\n",
            "40000/40000 [==============================] - 16s 395us/step - loss: 0.9781 - acc: 0.6665 - val_loss: 0.9459 - val_acc: 0.6764\n",
            "Epoch 69/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9745 - acc: 0.6669 - val_loss: 0.9562 - val_acc: 0.6739\n",
            "Epoch 70/100\n",
            "40000/40000 [==============================] - 16s 393us/step - loss: 0.9732 - acc: 0.6691 - val_loss: 0.9681 - val_acc: 0.6801\n",
            "Epoch 71/100\n",
            "40000/40000 [==============================] - 16s 398us/step - loss: 0.9779 - acc: 0.6674 - val_loss: 0.9374 - val_acc: 0.6822\n",
            "Epoch 72/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9698 - acc: 0.6706 - val_loss: 1.0884 - val_acc: 0.6334\n",
            "Epoch 73/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9695 - acc: 0.6690 - val_loss: 0.9397 - val_acc: 0.6858\n",
            "Epoch 74/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9660 - acc: 0.6687 - val_loss: 1.0309 - val_acc: 0.6558\n",
            "Epoch 75/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 0.9701 - acc: 0.6697 - val_loss: 0.9589 - val_acc: 0.6787\n",
            "Epoch 76/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 0.9558 - acc: 0.6743 - val_loss: 0.9749 - val_acc: 0.6756\n",
            "Epoch 77/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9678 - acc: 0.6708 - val_loss: 0.9554 - val_acc: 0.6796\n",
            "Epoch 78/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9639 - acc: 0.6718 - val_loss: 0.9214 - val_acc: 0.6867\n",
            "Epoch 79/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 0.9661 - acc: 0.6698 - val_loss: 0.9644 - val_acc: 0.6729\n",
            "Epoch 80/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9579 - acc: 0.6730 - val_loss: 0.9446 - val_acc: 0.6807\n",
            "Epoch 81/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 0.9540 - acc: 0.6741 - val_loss: 0.9979 - val_acc: 0.6548\n",
            "Epoch 82/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 0.9561 - acc: 0.6746 - val_loss: 0.9316 - val_acc: 0.6826\n",
            "Epoch 83/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9489 - acc: 0.6764 - val_loss: 0.9303 - val_acc: 0.6888\n",
            "Epoch 84/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 0.9573 - acc: 0.6774 - val_loss: 0.9292 - val_acc: 0.6903\n",
            "Epoch 85/100\n",
            "40000/40000 [==============================] - 16s 388us/step - loss: 0.9528 - acc: 0.6785 - val_loss: 0.9876 - val_acc: 0.6831\n",
            "Epoch 86/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9573 - acc: 0.6747 - val_loss: 0.9328 - val_acc: 0.6789\n",
            "Epoch 87/100\n",
            "40000/40000 [==============================] - 16s 393us/step - loss: 0.9543 - acc: 0.6753 - val_loss: 0.9630 - val_acc: 0.6825\n",
            "Epoch 88/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9578 - acc: 0.6758 - val_loss: 0.9157 - val_acc: 0.6897\n",
            "Epoch 89/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 0.9520 - acc: 0.6758 - val_loss: 0.9490 - val_acc: 0.6936\n",
            "Epoch 90/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 0.9522 - acc: 0.6777 - val_loss: 0.9324 - val_acc: 0.6809\n",
            "Epoch 91/100\n",
            "40000/40000 [==============================] - 16s 398us/step - loss: 0.9536 - acc: 0.6755 - val_loss: 1.0260 - val_acc: 0.6667\n",
            "Epoch 92/100\n",
            "40000/40000 [==============================] - 16s 390us/step - loss: 0.9516 - acc: 0.6767 - val_loss: 0.9185 - val_acc: 0.6920\n",
            "Epoch 93/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 0.9541 - acc: 0.6778 - val_loss: 0.9086 - val_acc: 0.6886\n",
            "Epoch 94/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 0.9510 - acc: 0.6782 - val_loss: 0.9382 - val_acc: 0.6818\n",
            "Epoch 95/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 0.9468 - acc: 0.6798 - val_loss: 1.0042 - val_acc: 0.6509\n",
            "Epoch 96/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 0.9476 - acc: 0.6791 - val_loss: 1.0804 - val_acc: 0.6478\n",
            "Epoch 97/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 0.9511 - acc: 0.6754 - val_loss: 0.9080 - val_acc: 0.6887\n",
            "Epoch 98/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 0.9467 - acc: 0.6779 - val_loss: 0.9063 - val_acc: 0.6929\n",
            "Epoch 99/100\n",
            "40000/40000 [==============================] - 16s 389us/step - loss: 0.9462 - acc: 0.6791 - val_loss: 0.9095 - val_acc: 0.6963\n",
            "Epoch 100/100\n",
            "40000/40000 [==============================] - 16s 391us/step - loss: 0.9440 - acc: 0.6805 - val_loss: 0.8926 - val_acc: 0.6906\n",
            "training time:  1559.5138347148895\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acMQveE4Rp36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the history training\n",
        "def plot_train_val(history, value=\"loss\"):\n",
        "    \"\"\"Print loss or accuracy\n",
        "    \"loss\" or \"acc\"\n",
        "    \"\"\"\n",
        "    train_value = history.history[value]\n",
        "    val_value = history.history[\"val_\"+value]\n",
        "\n",
        "    epochs = range(1, len(train_value) + 1)\n",
        "    # \"bo\" is for \"blue dot\"\n",
        "    plt.plot(epochs, train_value, 'r+', label='Training %s' % value)\n",
        "    # b is for \"solid blue line\"\n",
        "    plt.plot(epochs, val_value, 'b', label='Validation %s' % value)\n",
        "    plt.title('Training and validation %s' % value)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(value)\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-P9O-jwuky_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiQpWn80unr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYUnvbltupSE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "d45c89d8-0b84-4330-b3f4-15d55dac6316"
      },
      "source": [
        "plot_train_val(history, \"loss\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZOwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDRp2B0RuqG8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "128ef090-85f9-49d5-cfa3-c40bd66657fb"
      },
      "source": [
        "start = time.time()\n",
        "score, acc = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print(\"Evaluation time: \", time.time() - start)\n",
        "print(\"Score/accuracy: \", score, acc)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 145us/step\n",
            "Evaluation time:  1.4566154479980469\n",
            "Score/accuracy:  0.9059611549377441 0.6914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIPzkOEEusTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}